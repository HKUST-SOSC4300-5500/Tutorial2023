{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENSIM for word embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- THE GENSIM LIBRARY\n",
    "\n",
    "    - Gensim is an open-source python library for natural language processing.\n",
    "\n",
    "    - It was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. \n",
    "\n",
    "    - In the previous tutorial, we have seen how you can use this package to do topic modeling.\n",
    "    \n",
    "    - Here we use `gensim` for word embedding. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word embeddings can be used for:\n",
    "    - automated text tagging\n",
    "    - recommendation engines\n",
    "    - synonyms and search query expansion\n",
    "    - machine translation\n",
    "    - plain feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example\n",
    "\n",
    "<img src=\"1.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example\n",
    "\n",
    "<img src=\"2.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software for word embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Software for training and using word embeddings includes \n",
    "    - Tomas Mikolov's Word2vec, \n",
    "    - Stanford University's GloVe, GN-GloVe \n",
    "    - AllenNLP's ELMo,\n",
    "    - BERT\n",
    "    - fastText \n",
    "    - Gensim\n",
    "    - Indra and Deeplearning4j\n",
    "  \n",
    "    - Principal Component Analysis (PCA) and T-Distributed Stochastic Neighbour Embedding (t-SNE) are both used to reduce the dimensionality of word vector spaces and visualize word embeddings and clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re  # For preprocessing\n",
    "\n",
    "# from collections import defaultdict\n",
    "# from time import time  # To time our operations\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# import logging  # Setting up the loggings to monitor gensim\n",
    "# logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) CREATE A WORD2VEC MODEL\n",
    "\n",
    "- Training the model: Gensim Word2Vec Implementation:\n",
    "    - We use Gensim implementation of word2vec: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "\n",
    "common_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train a simpliest word embedding by yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_1 = Word2Vec(common_texts, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters of `Word2Vec`:\n",
    "\n",
    "- `vector_size`: # of dimensions of the embeddings and the default is 100.\n",
    "- `window`: The maximum distance between a target word and words around the target word. The default window is 5.\n",
    "- `min_count`: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.\n",
    "- `workers`: # of worker threads used to train the model; depends on your computer.\n",
    "- `sg`: The training algorithm, either CBOW(0) or skip-gram (1). The default training algorithm is CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model_1.wv['computer']  # 'wv': word to vector\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# new words? cannot handle\n",
    "model_1.wv['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.wv.most_similar('graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "model_1.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) WORD EMBEDDING Using a Real Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, I use [a dataset from Kaggle](https://www.kaggle.com/CooperUnion/cardataset). This cars dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The following dataframe shows the detail information of this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note what we are doing: **the structure of word embedding can not only be used on text but also on observation-feature dataframes**! Fundamentally it's about dimension reduction. \n",
    "\n",
    "In fact this is [also true for topic modeling (LDA)](https://www.journals.uchicago.edu/doi/10.1086/705331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRE-PROCESS WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning \n",
    "    - Removing the missing values;\n",
    "    - Lemmatizing;\n",
    "    - Removing the stopwords;\n",
    "    - Removes non-alphabetic characters: regular expression;\n",
    "    - Bigrams: We can use Gensim Phrases package to automatically detect common phrases (bigrams) from a list of sentences. https://radimrehurek.com/gensim/models/phrases.html\n",
    "\n",
    "         ```python\n",
    "         from gensim.models.phrases import Phrases, Phraser\n",
    "         ```\n",
    "         - As Phrases() takes a list of list of words as input:\n",
    "        ```python\n",
    "        sent = [row.split() for row in df_clean['clean']]\n",
    "        ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, I will not do the EDA and feature selection for the word2vec model for the sake of simplicity. \n",
    "<br> \n",
    "Genism word2Vec requires that a format of list of list for training where every document is contained in a list and every list contains list of tokens of that document. At first, we need to generate a format of list of list for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains list of features of that make model.\n",
    "\n",
    "To achieve these, we need to do the following data preprocessing steps:\n",
    "\n",
    "1. Create a new column for Make Model \n",
    "2. Generate a format of list of list for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size and Vehicle Style. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a new column for Make Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Maker_Model']= df['Make']+ \" \" + df['Model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate a format of list of list for each Make Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features from original dataset to form a new dataframe \n",
    "df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category',\n",
    "          'Vehicle Size', 'Vehicle Style', 'Maker_Model']]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each row, combine all the columns into one column\n",
    "df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1) \n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store them in the pandas dataframe\n",
    "df_clean = pd.DataFrame({'clean': df2}) \n",
    "\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['clean'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of list format of the custom corpus for gensim modeling \n",
    "sent = [row.split(',') for row in df_clean['clean']]\n",
    "# show the example of list of list format of the custom corpus for gensim modeling \n",
    "sent[:2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genism word2vec Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the genisim word2vec model with our own custom corpus\n",
    "model_2 = Word2Vec(sent, min_count=1, vector_size= 50, workers=3, window =3, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can obtain the word embedding directly from the training model\n",
    "model_2.wv['BMW 1 Series']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Similarities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could even use Word2vec to compute similarity between two make model in the vocabulary by invoking the model.similarity() and passing in the relvevant words. For instance,  model.similarity('Porsche 718 Cayman', 'Nissan Van') This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.wv.similarity('Porsche 718 Cayman', 'Nissan Van')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.wv.similarity('Porsche 718 Cayman', 'Mercedes-Benz SLK-Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above example, we can tell that Porsche 718 Cayman is more similar with Mercedes-Benz SLK-Class than Nissan Van. We also can use the build in function model.most_similar() to get a set of the most similar make models for a given make model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show the most similar vehicles for Mercedes-Benz SLK-Class : Default by eculidean distance \n",
    "model_2.wv.most_similar('Mercedes-Benz SLK-Class')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show the most similar vehicles for Toyota Camry : Default by eculidean distance \n",
    "model_2.wv.most_similar('Toyota Camry')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, Euclidian similarity cannot work well for the high-dimensional word vectors, This is because Euclidian similarity will increase as the number of dimensions increases even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors.  \n",
    "\n",
    "For Vector $A$ and $B$, the dot product is given by $ A \\cdot B = \\|A\\| \\|B\\| \\cos(\\theta)$\n",
    "\n",
    "The cosine similarity is given by $ \\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity measures the cosine of the angle between two vectors projected in a multi-dimensional space. Therefore, the cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at 0 degree \n",
    "angle. The following function shows how can we generate the most similar make model based on cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_distance (model, word,target_list , num) :\n",
    "    cosine_dict ={}\n",
    "    word_list = []\n",
    "    a = model.wv[word]\n",
    "    \n",
    "    for item in target_list :\n",
    "        if item != word :\n",
    "            b = model.wv[item]\n",
    "            cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "            cosine_dict[item] = cos_sim\n",
    "    dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order \n",
    "    \n",
    "    for item in dist_sort:\n",
    "        word_list.append((item[0], item[1]))\n",
    "    \n",
    "    return word_list[0:num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Maker_Model = list(df.Maker_Model.unique()) ## only get the unique Maker_Model_Year\n",
    "\n",
    "## Show the most similar Mercedes-Benz SLK-Class by cosine distance \n",
    "cosine_distance(model_2,'Mercedes-Benz SLK-Class',Maker_Model,5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.wv.most_similar('Mercedes-Benz SLK-Class')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Read pre-trained models\n",
    "\n",
    "As we said, it's usually far better to use some pre-trained embeddings instead of starting from scratches\n",
    "\n",
    "Read more: https://radimrehurek.com/gensim/models/keyedvectors.html\n",
    "\n",
    "\n",
    "I will read GloVe's pre-trained vectors here. Gensim offers download of some other pre-trained vectors. See\n",
    "https://github.com/RaRe-Technologies/gensim-data\n",
    "\n",
    "\n",
    "And a more complete pre-trained vector dataset can be found here\n",
    "http://vectors.nlpl.eu/repository/\n",
    "\n",
    "You may need to manually download them to your disk and let Gensim read in.\n",
    "\n",
    "\n",
    "Another source for Pre-trainned word and phrase vectors from Google: https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can find similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = word_vectors.most_similar(positive=['woman'])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may observe, girl and man are really different things. So we can use the king/queen and man/women analogy to find what's the similar word to \"woman\", if we hope to find pairs such as (king, queen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ man = woman + king - queen $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = word_vectors.most_similar(positive=['king', 'woman'], negative=['queen'])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ queen = king + woman - man $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = word_vectors.most_similar(positive=[ 'king', 'woman'], negative=['man'])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GoogleNews-vectors-negative300.bin.gz  as an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GoogleNews-vectors-negative300.bin.gz is pretty large and I won't upload it to GitHub. Please download it from [the official source](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g) to your local computer if you want to try out the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/Users/percychan/Tech/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "\n",
    "model_google = gensim.models.KeyedVectors.load_word2vec_format(file, binary=True,limit= 100000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = model_google['dog']\n",
    "print(dog.shape)\n",
    "print(dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with an out of dictionary word: Михаил (Michail)\n",
    "if 'Михаил' in model_google:\n",
    "    print(model_google['Михаил'].shape)\n",
    "else:\n",
    "    print('{0} is an out of dictionary word'.format('Михаил'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some predefined functions that show content related information for given words\n",
    "model_google.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = model_google['king'] - model_google['man'] + model_google['woman']\n",
    "model_google.most_similar([vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = model_google['Berlin'] - model_google['Germany'] + model_google['China']\n",
    "model_google.most_similar([vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = model_google['Germany'] - model_google['Berlin'] + model_google['Beijing']\n",
    "model_google.most_similar([vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = model_google['Messi'] - model_google['soccer'] + model_google['tennis']\n",
    "model_google.most_similar([vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_google.doesnt_match(\"breakfast economics dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_google.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_google.similarity('Harvard', 'Stanford')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_google.similarity('Cambridge', 'Oxford')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_google.most_similar('Harvard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_google.similarity('HKUST', 'HKU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_google.similarity('Economics', 'Sociology')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_google.similarity('Statistics', 'Economics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_google.similarity('Statistics', 'Sociology')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software \n",
    "- GloVe: https://nlp.stanford.edu/projects/glove/\n",
    "- Word2Vec: https://code.google.com/archive/p/word2vec/\n",
    "- Tensorflow Word2Vec tutorial: https://www.tensorflow.org/tutorials/text/word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
